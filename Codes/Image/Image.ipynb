{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pOND8uc4vh82",
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title\n",
        "!pip install gradio google-generativeai mediapipe opencv-python gtts googletrans==4.0.0-rc1\n",
        "!pip install gradio mediapipe opencv-python pillow numpy pandas tensorflow joblib gtts googletrans==4.0.0-rc1 edge-tts google-generativeai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import mediapipe as mp\n",
        "from tensorflow.keras.models import load_model\n",
        "import joblib\n",
        "from PIL import Image\n",
        "import google.generativeai as genai\n",
        "import re\n",
        "import asyncio\n",
        "from gtts import gTTS\n",
        "from googletrans import Translator\n",
        "import edge_tts\n",
        "\n",
        "def load_models_and_setup():\n",
        "    \"\"\"Load the sign language model and setup MediaPipe\"\"\"\n",
        "    print(\"üîÑ Loading models and setting up MediaPipe...\")\n",
        "\n",
        "    # Load model and label encoder\n",
        "    model = load_model(\"/content/Model (3).h5\")\n",
        "    label_encoder = joblib.load(\"/content/Model (3).pkl\")\n",
        "\n",
        "    # Setup MediaPipe\n",
        "    mp_hands = mp.solutions.hands\n",
        "    hands = mp_hands.Hands(\n",
        "        static_image_mode=True,\n",
        "        max_num_hands=2,\n",
        "        min_detection_confidence=0.5\n",
        "    )\n",
        "\n",
        "    print(\"‚úÖ Models loaded successfully!\")\n",
        "    return model, label_encoder, hands\n",
        "\n",
        "def process_images(model, label_encoder, hands, image_folder, output_csv_path):\n",
        "    \"\"\"Process all images in the folder and predict sign language\"\"\"\n",
        "    print(\"üîÑ Processing sign language images...\")\n",
        "\n",
        "    results = []\n",
        "\n",
        "    # Loop through all images\n",
        "    for filename in os.listdir(image_folder):\n",
        "        if filename.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
        "            image_path = os.path.join(image_folder, filename)\n",
        "\n",
        "            # Read image and convert to RGB\n",
        "            image = cv2.imread(image_path)\n",
        "            if image is None:\n",
        "                print(f\"‚ùå Could not read image: {filename}\")\n",
        "                continue\n",
        "\n",
        "            image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "            result = hands.process(image_rgb)\n",
        "\n",
        "            # Check if hands were detected\n",
        "            if result.multi_hand_landmarks:\n",
        "                keypoints = []\n",
        "\n",
        "                for hand_landmarks in result.multi_hand_landmarks:\n",
        "                    for lm in hand_landmarks.landmark:\n",
        "                        keypoints.extend([lm.x, lm.y, lm.z])\n",
        "\n",
        "                # If only one hand, pad to 126\n",
        "                if len(result.multi_hand_landmarks) == 1:\n",
        "                    keypoints.extend([-1.0] * 63)\n",
        "\n",
        "                if len(keypoints) == 126:\n",
        "                    X_input = np.array(keypoints).reshape(1, -1)\n",
        "                    prediction = model.predict(X_input)\n",
        "                    predicted_index = np.argmax(prediction)\n",
        "                    predicted_class = label_encoder.inverse_transform([predicted_index])[0]\n",
        "                    confidence = float(np.max(prediction))\n",
        "\n",
        "                    results.append({\n",
        "                        \"filename\": filename,\n",
        "                        \"predicted_class\": predicted_class,\n",
        "                        \"confidence\": confidence\n",
        "                    })\n",
        "                else:\n",
        "                    print(f\"Skipped {filename}: Invalid keypoints length = {len(keypoints)}\")\n",
        "            else:\n",
        "                print(f\"Skipped {filename}: No hands detected\")\n",
        "\n",
        "    # Save results to CSV\n",
        "    df = pd.DataFrame(results)\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "\n",
        "    print(f\"‚úÖ Predictions saved to {output_csv_path}\")\n",
        "    return df\n",
        "\n",
        "def generate_sentence_from_predictions(df, api_key):\n",
        "    \"\"\"Generate a natural sentence from predicted sign language words\"\"\"\n",
        "    print(\"üîÑ Generating sentence from predictions...\")\n",
        "\n",
        "    # Configure Gemini API\n",
        "    genai.configure(api_key=api_key)\n",
        "    model = genai.GenerativeModel(\"models/gemini-1.5-flash-latest\")\n",
        "\n",
        "    # Collect all predicted classes\n",
        "    predicted_words = df[\"predicted_class\"].tolist()\n",
        "    words = \", \".join(predicted_words)\n",
        "\n",
        "    prompt = (\n",
        "        f\"You are an emotionally intelligent sentence generator.\\n\"\n",
        "        f\"Use the following 3‚Äì5 words as inspiration: {words}.\\n\"\n",
        "        f\"Generate one short, natural-sounding sentence (8‚Äì12 words max).\\n\"\n",
        "        f\"Generate ONE and ONLY ONE grammatically correct sentence (8‚Äì12 words max).\\n\"\n",
        "        f\"Do not include explanations, multiple options, or repeated outputs.\\n\"\n",
        "        f\"End with appropriate punctuation (., ?, !). Return only the sentence.\\n\"\n",
        "        f\"Understand the intent behind the words, not just their surface form.\\n\"\n",
        "        f\"Choose the correct sentence style: request, statement, or question.\\n\"\n",
        "        f\"Be grammatically correct and use appropriate punctuation (!, ?, or .)\\n\"\n",
        "        f\"Examples:\\n\"\n",
        "        f\"  Words: Love Family ‚Üí Sentence: I really love my family.\\n\"\n",
        "        f\"  Words: Eat Food Now ‚Üí Sentence: Can we eat some food now?\\n\"\n",
        "        f\"  Words: Please Water ‚Üí Sentence: Please give me some water.\\n\"\n",
        "        f\"  Words: You Where ‚Üí Sentence: Where are you?\\n\"\n",
        "        f\"  Words: Smile Beautiful ‚Üí Sentence: Your smile is beautiful!\\n\"\n",
        "        f\"Now generate the sentence:\"\n",
        "    )\n",
        "\n",
        "    try:\n",
        "        response = model.generate_content(prompt)\n",
        "        text = response.text.strip()\n",
        "\n",
        "        # Extract the first valid sentence\n",
        "        sentences = re.findall(r'[^.!?]*[.!?]', text)\n",
        "        sentence = sentences[0].strip() if sentences else text\n",
        "\n",
        "        # Save sentence to file\n",
        "        with open(\"/content/generated_sentences.txt\", \"w\") as f:\n",
        "            f.write(sentence + \"\\n\")\n",
        "\n",
        "        print(f\"‚úÖ Sentence generated: {sentence}\")\n",
        "        return sentence\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error generating sentence: {e}\")\n",
        "        return \"ERROR\"\n",
        "\n",
        "async def generate_audio_files(sentence):\n",
        "    \"\"\"Generate audio files in both English and Chinese\"\"\"\n",
        "    print(\"üîÑ Generating audio files...\")\n",
        "\n",
        "    # Step 1: Translate to Chinese\n",
        "    translator = Translator()\n",
        "    translation = translator.translate(sentence, src=\"en\", dest=\"zh-cn\")\n",
        "    chinese_text = translation.text\n",
        "\n",
        "    print(f\"üåê Translation: {chinese_text}\")\n",
        "\n",
        "    # Step 2: Create output folder\n",
        "    output_folder = \"output_audio\"\n",
        "    os.makedirs(output_folder, exist_ok=True)\n",
        "\n",
        "    # Step 3: Generate English audio using gTTS\n",
        "    english_audio_path = os.path.join(output_folder, \"sentence_en.mp3\")\n",
        "    gTTS(text=sentence, lang=\"en\").save(english_audio_path)\n",
        "    print(f\"‚úÖ English audio saved to: {english_audio_path}\")\n",
        "\n",
        "    # Step 4: Generate Chinese audio using edge-tts\n",
        "    chinese_audio_path = os.path.join(output_folder, \"sentence_zh.mp3\")\n",
        "    communicate = edge_tts.Communicate(text=chinese_text, voice=\"zh-CN-XiaoxiaoNeural\")\n",
        "    await communicate.save(chinese_audio_path)\n",
        "    print(f\"‚úÖ Chinese audio saved to: {chinese_audio_path}\")\n",
        "\n",
        "    return english_audio_path, chinese_audio_path\n",
        "\n",
        "async def main():\n",
        "    \"\"\"Main function to run the complete pipeline\"\"\"\n",
        "    print(\"üöÄ Starting Sign Language Processing Pipeline\\n\")\n",
        "\n",
        "    # Configuration\n",
        "    IMAGE_FOLDER = \"/content/testimages\"\n",
        "    OUTPUT_CSV_PATH = \"/content/prediction.csv\"\n",
        "    GEMINI_API_KEY = \"AIzaSyC4h_QLZOZMUzQRzemTcwPSfjdBQO1I2Ac\"\n",
        "\n",
        "    try:\n",
        "        # Step 1: Load models and setup MediaPipe\n",
        "        model, label_encoder, hands = load_models_and_setup()\n",
        "\n",
        "        # Step 2: Process images and predict sign language\n",
        "        df = process_images(model, label_encoder, hands, IMAGE_FOLDER, OUTPUT_CSV_PATH)\n",
        "\n",
        "        if df.empty:\n",
        "            print(\"‚ùå No valid predictions found. Exiting...\")\n",
        "            return\n",
        "\n",
        "        print(f\"\\nüìä Predictions Summary:\")\n",
        "        print(df.head())\n",
        "\n",
        "        # Step 3: Generate sentence from predictions\n",
        "        sentence = generate_sentence_from_predictions(df, GEMINI_API_KEY)\n",
        "\n",
        "        if sentence == \"ERROR\":\n",
        "            print(\"‚ùå Failed to generate sentence. Exiting...\")\n",
        "            return\n",
        "\n",
        "        # Step 4: Generate audio files\n",
        "        english_path, chinese_path = await generate_audio_files(sentence)\n",
        "\n",
        "        print(f\"\\nüéâ Pipeline completed successfully!\")\n",
        "        print(f\"üìù Generated sentence: {sentence}\")\n",
        "        print(f\"üîä English audio: {english_path}\")\n",
        "        print(f\"üîä Chinese audio: {chinese_path}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Pipeline error: {e}\")\n",
        "\n",
        "# Run the pipeline\n",
        "if __name__ == \"__main__\":\n",
        "    # Handle both Jupyter and regular Python environments\n",
        "    try:\n",
        "        # Check if we're in a Jupyter notebook\n",
        "        if asyncio.get_event_loop().is_running():\n",
        "            # In Jupyter, create a new event loop in a thread\n",
        "            import nest_asyncio\n",
        "            nest_asyncio.apply()\n",
        "            asyncio.run(main())\n",
        "        else:\n",
        "            # In regular Python environment\n",
        "            asyncio.run(main())\n",
        "    except RuntimeError:\n",
        "        # Fallback for different environments\n",
        "        asyncio.run(main())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 486
        },
        "id": "TaqBNgGkwfpP",
        "outputId": "05a8fc10-cf2f-46f5-9315-a178882267ef"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üöÄ Starting Sign Language Processing Pipeline\n",
            "\n",
            "üîÑ Loading models and setting up MediaPipe...\n",
            "‚úÖ Models loaded successfully!\n",
            "üîÑ Processing sign language images...\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 136ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "‚úÖ Predictions saved to /content/prediction.csv\n",
            "\n",
            "üìä Predictions Summary:\n",
            "             filename predicted_class  confidence\n",
            "0  Water_0616 (1).jpg           Water    0.999494\n",
            "1         Me_1145.jpg              Me    0.999987\n",
            "2     Please_0344.jpg          Please    0.999980\n",
            "üîÑ Generating sentence from predictions...\n",
            "‚úÖ Sentence generated: Please, give me some water.\n",
            "üîÑ Generating audio files...\n",
            "üåê Translation: ËØ∑ÁªôÊàë‰∏Ä‰∫õÊ∞¥„ÄÇ\n",
            "‚úÖ English audio saved to: output_audio/sentence_en.mp3\n",
            "‚úÖ Chinese audio saved to: output_audio/sentence_zh.mp3\n",
            "\n",
            "üéâ Pipeline completed successfully!\n",
            "üìù Generated sentence: Please, give me some water.\n",
            "üîä English audio: output_audio/sentence_en.mp3\n",
            "üîä Chinese audio: output_audio/sentence_zh.mp3\n"
          ]
        }
      ]
    }
  ]
}